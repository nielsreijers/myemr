{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import groupby\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import myfunctions\n",
    "import myplot\n",
    "import mylearn\n",
    "import myhelpers\n",
    "\n",
    "# Home, quiet, long measurement, (Space,Return,Back,Tilde), 903 keystrokes with 731 > 0.1\n",
    "noweb_logitech1 = ('winform/winform-1619400514590', -80, True, 'home,long,4key') # Still lost some samples after about 100 seconds\n",
    "\n",
    "# Home, quiet, short measurement taken a little while after noweb_logitech1, (Space,Return,Back,Tilde), 42 keystrokes with 19 > 0.1\n",
    "noweb_logitech2 = ('winform/winform-1619417452127', 70, False, 'home,short,4key')\n",
    "\n",
    "# NTU lab, long, (Space,Return,Back,Tilde)\n",
    "noweb_logitech3 = ('winform/winform-1619490434553', 70, False, 'NTU,long,4key')\n",
    "\n",
    "# NTU lab, short, (Space,Return,Back,Tilde)\n",
    "noweb_logitech4 = ('winform/winform-1619497973221', 70, False, 'NTU,short,4key')\n",
    "\n",
    "# NTU lab, short, AC noise, (Space,Return,Back,Tilde)\n",
    "noweb_logitech5 = ('winform/winform-1619498839675', 70, False, 'NTU,short,4key,AC')\n",
    "\n",
    "# NTU lab, short, AC noise, typing some text naturally\n",
    "noweb_logitech6 = ('winform/winform-1619499075929', 70, False, 'NTU,short,4key,AC')\n",
    "\n",
    "\n",
    "def getData2(x, keystroke_min_peak_level):\n",
    "    if len(x) == 4:\n",
    "        path, sync_adjustment, adjust_missing_samples, name = x\n",
    "    else:\n",
    "        path, sync_adjustment, adjust_missing_samples = x\n",
    "        name = ''\n",
    "    d = myfunctions.getData(path,\n",
    "                            sync_adjustment=sync_adjustment,\n",
    "                            adjust_missing_samples=adjust_missing_samples,\n",
    "                            keystroke_min_peak_level=keystroke_min_peak_level)\n",
    "    d['name'] = name\n",
    "    print(myhelpers.getListGroupPercentages([x[0] for x in d['keystrokes']], '\\n'))\n",
    "    return d\n",
    "\n",
    "d1 = getData2(noweb_logitech1, keystroke_min_peak_level=0.05)\n",
    "d2 = getData2(noweb_logitech2, keystroke_min_peak_level=0.05)\n",
    "d3 = getData2(noweb_logitech3, keystroke_min_peak_level=0.05)\n",
    "d4 = getData2(noweb_logitech4, keystroke_min_peak_level=0.05)\n",
    "d5 = getData2(noweb_logitech5, keystroke_min_peak_level=0.05)\n",
    "d6 = getData2(noweb_logitech6, keystroke_min_peak_level=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylearn.addFeatures(d1)\n",
    "mylearn.addFeatures(d2)\n",
    "mylearn.addFeatures(d3)\n",
    "mylearn.addFeatures(d4)\n",
    "mylearn.addFeatures(d5)\n",
    "mylearn.addFeatures(d6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below copied and modified from https://towardsdatascience.com/siamese-networks-line-by-line-explanation-for-beginners-55b8be1d2fc6\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXY(data):\n",
    "    x = data['mfcc']\n",
    "    y = np.asarray(data['keystroke_labels'])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def getCatlist(y):\n",
    "    keyindexes = [(key,i) for i,key in enumerate(y)]\n",
    "    sortedkeyindexes = sorted(keyindexes, key=lambda v: v[0])\n",
    "    groupedkeyindexes = groupby(sortedkeyindexes, lambda v: v[0])\n",
    "    catlist = dict([(key, [v[1] for v in group]) for key, group in groupedkeyindexes])\n",
    "\n",
    "    return catlist\n",
    "\n",
    "\n",
    "def getTestTrain(data, samples_per_training_key=5, number_of_training_keys=None):\n",
    "    x, y = getXY(d1)\n",
    "    catlist = getCatlist(y)\n",
    "\n",
    "    keys = set(y)\n",
    "    number_of_training_keys = 2\n",
    "    keys_train = random.sample(keys, number_of_training_keys)\n",
    "    keys_test = list(keys - set(keys_train))\n",
    "\n",
    "    samples_per_training_key = 4\n",
    "    idxs_train = sum([random.sample(catlist[key], samples_per_training_key) for key in keys_train], [])\n",
    "    x_train = x[idxs_train]\n",
    "    y_train = y[idxs_train]\n",
    "\n",
    "    idxs_test = sum([catlist[key] for key in keys_test], [])\n",
    "    x_test = x[idxs_test]\n",
    "    y_test = y[idxs_test]\n",
    "       \n",
    "    print(f'X&Y shape of training data: {x_train.shape} and {y_train.shape}. Keys: {len(keys_train)}.')\n",
    "    print(f'X&Y shape of testing data: {x_test.shape} and {y_test.shape}. Keys: {len(keys_test)}.')\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "x_train, y_train, x_test, y_test = getTestTrain(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(x, y, batchsize):\n",
    "    catlist = getCatlist(y)\n",
    "    keys = list(catlist.keys())\n",
    "\n",
    "    batch_y = np.zeros(batchsize)\n",
    "    batch_y[int(batchsize/2):] = 1\n",
    "    np.random.shuffle(batch_y)\n",
    "    \n",
    "    batch_x = [[], []]\n",
    "    for i in range(0, batchsize):\n",
    "        key = random.choice(keys)\n",
    "        batch_x[0].append(x[random.choice(catlist[key])])\n",
    "        #If train_y has 0 pick from the same class, else pick from any other class\n",
    "        if batch_y[i]==0:\n",
    "            batch_x[1].append(x[random.choice(catlist[key])])\n",
    "        else:\n",
    "            other_key = random.choice([k for k in keys if k != key])\n",
    "            batch_x[1].append(x[random.choice(catlist[other_key])])\n",
    "    batch_x = np.asarray(batch_x)\n",
    "    print(f'X&Y shape of batch data: {batch_x.shape} and {batch_y.shape}.')\n",
    "\n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a sequential model\n",
    "input_shape=(32, 21, 1)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "\n",
    "W_init = keras.initializers.RandomNormal(mean = 0.0, stddev = 1e-2)\n",
    "b_init = keras.initializers.RandomNormal(mean = 0.5, stddev = 1e-2)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(64, (2,4), activation='relu', input_shape=input_shape, kernel_initializer=W_init, bias_initializer=b_init, kernel_regularizer=l2(2e-4)),\n",
    "    keras.layers.MaxPooling2D(2, 2),\n",
    "    keras.layers.Conv2D(128, (7,7), activation='relu', kernel_initializer=W_init, bias_initializer=b_init, kernel_regularizer=l2(2e-4)),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "#     keras.layers.Conv2D(128, (4,4), activation='relu', kernel_initializer=W_init, bias_initializer=b_init, kernel_regularizer=l2(2e-4)),\n",
    "#     keras.layers.MaxPooling2D(2,2),\n",
    "#     keras.layers.Conv2D(256, (4,4), activation='relu', kernel_initializer=W_init, bias_initializer=b_init, kernel_regularizer=l2(2e-4)),\n",
    "#     keras.layers.MaxPooling2D(2,2),\n",
    "#     keras.layers.Flatten(),\n",
    "#     keras.layers.Dense(4096, activation='sigmoid', kernel_initializer=W_init, bias_initializer=b_init)\n",
    "])\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "# L1_distance = lambda x: K.abs(x[0] - x[1])\n",
    "# print(type(L1_distance))\n",
    "# both = merge([encoded_l, encoded_r], mode=L1_distance, output_shape = lambda x:x[0])\n",
    "subtracted = keras.layers.Subtract()([encoded_l, encoded_r])\n",
    "prediction = Dense(1, activation='sigmoid', bias_initializer=b_init)(subtracted)\n",
    "#siamese_net = Model(input=[left_input, right_input], output=prediction) # NR: I changed this\n",
    "siamese_net = Model([left_input, right_input], prediction)\n",
    "\n",
    "optimizer= Adam(learning_rate=0.0006)\n",
    "siamese_net.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "optimizer = Adam(lr = 0.00006)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "\n",
    "plot_model(siamese_net, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
