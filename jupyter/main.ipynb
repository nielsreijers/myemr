{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import myfunctions\n",
    "import myplot\n",
    "import mylearn\n",
    "import myhelpers\n",
    "\n",
    "logitech_step1_fast = ('logitech-fast/niels-step_1-1617765899871', 0)\n",
    "logitech_step1_slow = ('logitech-slow/niels-step_1-1617765658270', 60, True)\n",
    "lenovo_slow = ('lenovo-slow-easy/niels-step_5-1617952576650', 0)\n",
    "lenovo_slow2 = ('lenovo-slow-easy/niels-step_5-1618197371437', 0)\n",
    "lenovo_hard = ('lenovo-hard/niels-step_1-1618199054005', 0)\n",
    "noweb1 = ('winform/winform-1618904428438', 60, False)\n",
    "noweb2 = ('winform/winform-1619147742756', 75, False)\n",
    "\n",
    "# Home, quiet, long measurement, (Space,Return,Back,Tilde), 903 keystrokes with 731 > 0.1\n",
    "noweb_logitech1 = ('winform/winform-1619400514590', -80, True, 'home,long,4key') # Still lost some samples after about 100 seconds\n",
    "\n",
    "# Home, quiet, short measurement taken a little while after noweb_logitech1, (Space,Return,Back,Tilde), 42 keystrokes with 19 > 0.1\n",
    "noweb_logitech2 = ('winform/winform-1619417452127', 70, False, 'home,short,4key')\n",
    "\n",
    "# NTU lab, long, (Space,Return,Back,Tilde)\n",
    "noweb_logitech3 = ('winform/winform-1619490434553', 70, False, 'NTU,long,4key')\n",
    "\n",
    "# NTU lab, short, (Space,Return,Back,Tilde)\n",
    "noweb_logitech4 = ('winform/winform-1619497973221', 70, False, 'NTU,short,4key')\n",
    "\n",
    "# NTU lab, short, AC noise, (Space,Return,Back,Tilde)\n",
    "noweb_logitech5 = ('winform/winform-1619498839675', 70, False, 'NTU,short,4key,AC')\n",
    "\n",
    "# NTU lab, short, AC noise, typing some text naturally\n",
    "noweb_logitech6 = ('winform/winform-1619499075929', 70, False, 'NTU,short,4key,AC')\n",
    "\n",
    "\n",
    "def getData2(x, keystroke_min_peak_level):\n",
    "    if len(x) == 4:\n",
    "        path, sync_adjustment, adjust_missing_samples, name = x\n",
    "    else:\n",
    "        path, sync_adjustment, adjust_missing_samples = x\n",
    "        name = ''\n",
    "    d = myfunctions.getData(path,\n",
    "                            sync_adjustment=sync_adjustment,\n",
    "                            adjust_missing_samples=adjust_missing_samples,\n",
    "                            keystroke_min_peak_level=keystroke_min_peak_level)\n",
    "    d['name'] = name\n",
    "    print(myhelpers.getListGroupPercentages([x[0] for x in d['keystrokes']], '\\n'))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = getData2(noweb_logitech1, keystroke_min_peak_level=0.05)\n",
    "d2 = getData2(noweb_logitech2, keystroke_min_peak_level=0.05)\n",
    "d3 = getData2(noweb_logitech3, keystroke_min_peak_level=0.05)\n",
    "d4 = getData2(noweb_logitech4, keystroke_min_peak_level=0.05)\n",
    "d5 = getData2(noweb_logitech5, keystroke_min_peak_level=0.05)\n",
    "d6 = getData2(noweb_logitech6, keystroke_min_peak_level=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=d6\n",
    "\n",
    "# myplot.plotLostSamples(d)\n",
    "\n",
    "# myplot.plotKeystrokeContext(d, 20, 70)\n",
    "\n",
    "# for i in range(21, 31):\n",
    "#     myplot.plotKeystroke(d, i)\n",
    "\n",
    "# d.keys()\n",
    "\n",
    "# set([x[3] for x in d['down_events']])\n",
    "\n",
    "# for i in range(10, 17):\n",
    "#     myplot.plotKeystroke(d, i)\n",
    "\n",
    "d=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This worked pretty well (lenovo)\n",
    "# mylearn.testClustering(getData2(noweb2, keystroke_min_peak_level=0.1),\n",
    "#     keep=None,\n",
    "#     features=['mfcc_mean', 'mfcc_std'])\n",
    "\n",
    "# Half-decent result using larger dataset (6 min, ~800 keystrokes), but still an easy case with just 4 keys (Return, Space, Tilde, Backspace)\n",
    "mylearn.testClustering(getData2(noweb_logitech1, keystroke_min_peak_level=0.1),\n",
    "    keep=None,\n",
    "    features=['mfcc_mean', 'mfcc_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylearn.testClustering(d,\n",
    "    keep=None,\n",
    "    features=[\n",
    "        'mfcc_mean',\n",
    "        'mfcc_std',\n",
    "#         'mfcc_max',\n",
    "#         'mfcc_argmax_time',\n",
    "#         'mfcc_argmax_channel'\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNumberOfClusters(features):\n",
    "    x = list(range(2, 20))\n",
    "    scores = []\n",
    "    for n_clusters in tqdm(x):\n",
    "        kmeans = sklearn.cluster.KMeans(n_clusters)\n",
    "        kmeans.fit(features)\n",
    "        kmeans_score = kmeans.score(features)\n",
    "        clustering = kmeans.predict(features)\n",
    "        kmeans_silhouette_score = silhouette_score(features, clustering)\n",
    "\n",
    "        gm = sklearn.mixture.GaussianMixture(n_clusters)\n",
    "        gm.fit(features)\n",
    "        gm_score = gm.score(features)\n",
    "        gm_bic_score = gm.bic(features)\n",
    "        \n",
    "        scores.append((kmeans_score, kmeans_silhouette_score, gm_score, gm_bic_score))\n",
    "    return x, scores\n",
    "\n",
    "features = mylearn.getConcatenatedFeatures(d, ['mfcc_max', 'mfcc_mean', 'mfcc_std'])\n",
    "number_of_clusters,scores = findNumberOfClusters(mylearn.getConcatenatedFeatures(d, ['mfcc_max', 'mfcc_mean', 'mfcc_std']))\n",
    "\n",
    "for (i, s) in enumerate(['kmeans_score', 'kmeans_silhouette_score', 'gm_score', 'gm_bic_score']):\n",
    "    plt.figure()\n",
    "    plt.title(s)\n",
    "    plt.scatter(number_of_clusters, [x[i] for x in scores])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "sr = d['sr']\n",
    "srms = int(sr/1000)\n",
    "window_length = int(10*srms)\n",
    "hop_length = int(2.5*srms)\n",
    "wav = d['keystroke_wavs'][0]\n",
    "print( len(wav) )\n",
    "print( len(wav) / srms )\n",
    "print( librosa.feature.spectral_centroid(wav, sr, win_length=window_length, hop_length=hop_length).shape )\n",
    "print( librosa.feature.chroma_stft(wav, sr, win_length=window_length, hop_length=hop_length).shape )\n",
    "print( librosa.feature.zero_crossing_rate(wav, frame_length=window_length, hop_length=hop_length).shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot.plotPCA(d1, ['mfcc_max', 'mfcc_mean'])\n",
    "# 'mfcc_max', 'mfcc_mean', 'mfcc_std', 'mfcc_argmax_time', 'mfcc_argmax_channel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot.plotPCA(d3, ['mfcc_max', 'mfcc_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d=getData2(('winform-1619490289360', 0, False), 0)\n",
    "myplot.plotMatplotlib(d)\n",
    "# myplot.plotWaveAndKeys(d)\n",
    "# getData2(x, keystroke_min_peak_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 100 training samples seems about enough (for 4 keys)\n",
    "# [(training_sample_count, logRegTrain(d1, training_sample_count=training_sample_count)[2])\n",
    "#  for training_sample_count in [10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 400, 500, 600, 700, 800]\n",
    "# ]\n",
    "\n",
    "# scaler, lr = logRegTrain(d1)\n",
    "# logRegTest(d3, scaler, lr)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def test(d_train, d_test, featurenames, classifier, labels_filter=None):\n",
    "    def dname(data):\n",
    "        if type(data) != list:\n",
    "            data = [data]\n",
    "        return '+'.join([d['name'] for d in data])\n",
    "    scaler, classifier = mylearn.classifierTrain(d_train, featurenames=featurenames, classifier=classifier, labels_filter=labels_filter)\n",
    "    score = mylearn.classifierTest(scaler, classifier, d_test, labels_filter=labels_filter, featurenames=featurenames)\n",
    "    if labels_filter == None:\n",
    "        print (f'Score {score:1.3f} when trained on {dname(d_train):30} and tested using {dname(d_test):20}')\n",
    "    else:\n",
    "        print (f'Score {score:1.3f} when trained on {dname(d_train):30} and tested using {dname(d_test):20} with labels filter {labels_filter}')\n",
    "    return score\n",
    "\n",
    "# default solver is incredibly slow which is why it was changed to 'lbfgs'\n",
    "def testAll(featurenames, classifierFactory=lambda: LogisticRegression(solver = 'lbfgs')):\n",
    "    scores = []\n",
    "    scores.append(test(d1, d2, featurenames=featurenames, classifier=classifierFactory()))\n",
    "    scores.append(test(d1, d3, featurenames=featurenames, classifier=classifierFactory()))\n",
    "    scores.append(test(d1, d4, featurenames=featurenames, classifier=classifierFactory()))\n",
    "    scores.append(test(d1, d5, featurenames=featurenames, classifier=classifierFactory()))\n",
    "    scores.append(test(d1, d6, featurenames=featurenames, classifier=classifierFactory()))\n",
    "    print()\n",
    "    scores.append(test(d3, d1, featurenames=featurenames, classifier=classifierFactory()))\n",
    "    scores.append(test(d3, d2, featurenames=featurenames, classifier=classifierFactory()))\n",
    "    scores.append(test(d3, d4, featurenames=featurenames, classifier=classifierFactory()))\n",
    "    scores.append(test(d3, d5, featurenames=featurenames, classifier=classifierFactory()))\n",
    "    scores.append(test(d3, d6, featurenames=featurenames, classifier=classifierFactory()))\n",
    "    print()\n",
    "    scores.append(test([d1, d3], d2, featurenames=featurenames, classifier=classifierFactory()))\n",
    "    scores.append(test([d1, d3], d4, featurenames=featurenames, classifier=classifierFactory()))\n",
    "    scores.append(test([d1, d3], d5, featurenames=featurenames, classifier=classifierFactory()))\n",
    "    scores.append(test([d1, d3], d6, featurenames=featurenames, classifier=classifierFactory()))\n",
    "    print()\n",
    "    scores.append(test(d1, d2, featurenames=featurenames, classifier=classifierFactory(), labels_filter=['Space']))\n",
    "    scores.append(test(d1, d3, featurenames=featurenames, classifier=classifierFactory(), labels_filter=['Space']))\n",
    "    scores.append(test(d1, d4, featurenames=featurenames, classifier=classifierFactory(), labels_filter=['Space']))\n",
    "    scores.append(test(d1, d5, featurenames=featurenames, classifier=classifierFactory(), labels_filter=['Space']))\n",
    "    scores.append(test(d1, d6, featurenames=featurenames, classifier=classifierFactory(), labels_filter=['Space']))\n",
    "    print()\n",
    "    scores.append(test(d3, d1, featurenames=featurenames, classifier=classifierFactory(), labels_filter=['Space']))\n",
    "    scores.append(test(d3, d2, featurenames=featurenames, classifier=classifierFactory(), labels_filter=['Space']))\n",
    "    scores.append(test(d3, d4, featurenames=featurenames, classifier=classifierFactory(), labels_filter=['Space']))\n",
    "    scores.append(test(d3, d5, featurenames=featurenames, classifier=classifierFactory(), labels_filter=['Space']))\n",
    "    scores.append(test(d3, d6, featurenames=featurenames, classifier=classifierFactory(), labels_filter=['Space']))\n",
    "    print()\n",
    "    scores.append(test([d1, d3], d2, featurenames=featurenames, classifier=classifierFactory(), labels_filter=['Space']))\n",
    "    scores.append(test([d1, d3], d4, featurenames=featurenames, classifier=classifierFactory(), labels_filter=['Space']))\n",
    "    scores.append(test([d1, d3], d5, featurenames=featurenames, classifier=classifierFactory(), labels_filter=['Space']))\n",
    "    scores.append(test([d1, d3], d6, featurenames=featurenames, classifier=classifierFactory(), labels_filter=['Space']))\n",
    "    print (f'Mean score: {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['mfcc_flattened'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testAll(['mfcc_flattened'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testAll(['mfcc_flattened'], lambda: KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testAll(['mfcc_flattened'], lambda: MLPClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testAll(['mfcc_flattened'], lambda: GaussianNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def specshowRandomMfccFeatures(data, key):\n",
    "    indexes = [i for i, x in enumerate(data['keystroke_labels']) if x == key]\n",
    "    index = random.choice(indexes)\n",
    "    print (f'Key {data[\"keystroke_labels\"][index]} at index {index}')\n",
    "    librosa.display.specshow(data['normalised_mfcc_features'][index])\n",
    "\n",
    "specshowRandomMfccFeatures(d1, 'Space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specshowRandomMfccFeatures(d1, 'Oemtilde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(d1[\"keystroke_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = d1['keystrokes'][0][1]\n",
    "sr = d1['sr']\n",
    "print(w.shape)\n",
    "print(sr)\n",
    "import librosa\n",
    "print(librosa.feature.mfcc(w, sr, n_mfcc=32, n_fft=len(w), hop_length=len(w)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
